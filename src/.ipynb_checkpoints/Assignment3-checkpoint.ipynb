{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GrabCut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from collections import namedtuple\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import igraph as ig\n",
    "import logging\n",
    "import sys\n",
    "from tqdm.auto import tqdm as tq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = namedtuple('_', ('FIX', 'UNK', 'FG', 'BG'))(1, 0, 1, 0)\n",
    "NUM_GMM_COMP = 5\n",
    "GAMMA = 50\n",
    "LAMDA = 9 * GAMMA\n",
    "NUM_ITERS = 3\n",
    "TOL = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventHandler:\n",
    "    \"\"\"\n",
    "    Class for handling user input during segmentation iterations \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, flags, img, _types, _alphas, colors):\n",
    "        \n",
    "        self.FLAGS = flags\n",
    "        self.ix = -1\n",
    "        self.iy = -1\n",
    "        self.img = img\n",
    "        self.img2 = self.img.copy()\n",
    "        self._types = _types\n",
    "        self._alphas = _alphas\n",
    "        self.COLORS = colors\n",
    "\n",
    "    @property\n",
    "    def image(self):\n",
    "        return self.img\n",
    "    \n",
    "    @image.setter\n",
    "    def image(self, img):\n",
    "        self.img = img\n",
    "        \n",
    "    @property\n",
    "    def types(self):\n",
    "        return self._types\n",
    "\n",
    "    @types.setter\n",
    "    def types(self, _types):\n",
    "        self._types = _types\n",
    "    \n",
    "    @property\n",
    "    def alphas(self):\n",
    "        return self._alphas\n",
    "\n",
    "    @alphas.setter\n",
    "    def alphas(self, _alphas):\n",
    "        self._alphas = _alphas\n",
    "    \n",
    "    @property\n",
    "    def flags(self):\n",
    "        return self.FLAGS \n",
    "    \n",
    "    @flags.setter\n",
    "    def flags(self, flags):\n",
    "        self.FLAGS = flags\n",
    "    \n",
    "    def handler(self, event, x, y, flags, param):\n",
    "\n",
    "        # Draw the rectangle first\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            if self.FLAGS['rect_over'] == False:\n",
    "                self.FLAGS['DRAW_RECT'] = True\n",
    "                self.ix, self.iy = x,y\n",
    "\n",
    "        elif event == cv2.EVENT_MOUSEMOVE:\n",
    "            if self.FLAGS['DRAW_RECT'] == True:\n",
    "                self.img = self.img2.copy()\n",
    "                cv2.rectangle(self.img, (self.ix, self.iy), (x, y), self.COLORS['BLUE'], 2)\n",
    "                self.FLAGS['RECT'] = (min(self.ix, x), min(self.iy, y), abs(self.ix - x), abs(self.iy - y))\n",
    "                self.FLAGS['rect_or_mask'] = 0\n",
    "\n",
    "        elif event == cv2.EVENT_LBUTTONUP:\n",
    "            if self.FLAGS['rect_over'] == False:\n",
    "                self.FLAGS['DRAW_RECT'] = False\n",
    "                self.FLAGS['rect_over'] = True\n",
    "                cv2.rectangle(self.img, (self.ix, self.iy), (x, y), self.COLORS['BLUE'], 2)\n",
    "                self.FLAGS['RECT'] = (min(self.ix, x), min(self.iy, y), abs(self.ix - x), abs(self.iy - y))\n",
    "                self.FLAGS['rect_or_mask'] = 0\n",
    "\n",
    "                # initialise types and alphas\n",
    "                temp = np.zeros(self._types.shape, dtype=np.uint8)\n",
    "                rect = self.FLAGS['RECT']\n",
    "                temp[rect[1]:rect[1]+rect[3], rect[0]:rect[0]+rect[2]] = 1\n",
    "                self._types[temp == 0] = con.FIX\n",
    "                self._alphas[rect[1]:rect[1]+rect[3], rect[0]:rect[0]+rect[2]] = con.FG\n",
    "\n",
    "        \n",
    "        # Draw strokes for refinement \n",
    "\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            if self.FLAGS['rect_over'] == True:\n",
    "                self.FLAGS['DRAW_STROKE'] = True\n",
    "                cv2.circle(self.img, (x,y), 3, self.FLAGS['value']['color'], -1)\n",
    "                cv2.circle(self._alphas, (x,y), 3, self.FLAGS['value']['val'], -1)\n",
    "                cv2.circle(self._types, (x,y), 3, con.FIX, -1)\n",
    "\n",
    "        elif event == cv2.EVENT_MOUSEMOVE:\n",
    "            if self.FLAGS['DRAW_STROKE'] == True:\n",
    "                cv2.circle(self.img, (x, y), 3, self.FLAGS['value']['color'], -1)\n",
    "                cv2.circle(self._alphas, (x,y), 3, self.FLAGS['value']['val'], -1)\n",
    "                cv2.circle(self._types, (x,y), 3, con.FIX, -1)\n",
    "\n",
    "        elif event == cv2.EVENT_LBUTTONUP:\n",
    "            if self.FLAGS['DRAW_STROKE'] == True:\n",
    "                self.FLAGS['DRAW_STROKE'] = False\n",
    "                cv2.circle(self.img, (x, y), 3, self.FLAGS['value']['color'], -1)\n",
    "                cv2.circle(self._alphas, (x,y), 3, self.FLAGS['value']['val'], -1)\n",
    "                cv2.circle(self._types, (x,y), 3, con.FIX, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(filename, n_components=NUM_GMM_COMP, gamma=GAMMA, lamda=LAMDA,\n",
    "        num_iters=NUM_ITERS, tol=TOL, connect_diag=True):\n",
    "    \"\"\"\n",
    "    Main loop that implements GrabCut. \n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    filename (str) : Path to image\n",
    "    \"\"\"\n",
    "    \n",
    "    COLORS = {\n",
    "    'BLACK' : [0,0,0],\n",
    "    'RED'   : [0, 0, 255],\n",
    "    'GREEN' : [0, 255, 0],\n",
    "    'BLUE'  : [255, 0, 0],\n",
    "    'WHITE' : [255,255,255]\n",
    "    }\n",
    "\n",
    "    DRAW_BG = {'color' : COLORS['BLACK'], 'val' : con.BG}\n",
    "    DRAW_FG = {'color' : COLORS['WHITE'], 'val' : con.FG}\n",
    "\n",
    "    FLAGS = {\n",
    "        'RECT' : (0, 0, 1, 1),\n",
    "        'DRAW_STROKE': False,         # flag for drawing strokes\n",
    "        'DRAW_RECT' : False,          # flag for drawing rectangle\n",
    "        'rect_over' : False,          # flag to check if rectangle is  drawn\n",
    "        'rect_or_mask' : -1,          # flag for selecting rectangle or stroke mode\n",
    "        'value' : DRAW_FG,            # drawing strokes initialized to mark foreground\n",
    "    }\n",
    "\n",
    "    img = cv2.imread(filename)\n",
    "    img2 = img.copy()\n",
    "    types = np.zeros(img.shape[:2], dtype = np.uint8)  # whether a pixel is known or unknown\n",
    "    alphas = np.zeros(img.shape[:2], dtype = np.uint8) # mask is a binary array with : 0 - background pixels\n",
    "                                                       #                               1 - foreground pixels \n",
    "    output = np.zeros(img.shape, np.uint8)             # output image to be shown\n",
    "\n",
    "    # Input and segmentation windows\n",
    "    cv2.namedWindow('Input Image')\n",
    "    cv2.namedWindow('Segmented output')\n",
    "    \n",
    "    EventObj = EventHandler(FLAGS, img, types, alphas, COLORS)\n",
    "    cv2.setMouseCallback('Input Image', EventObj.handler)\n",
    "    cv2.moveWindow('Input Image', img.shape[1] + 10, 90)\n",
    "\n",
    "    while(1):\n",
    "        \n",
    "        img = EventObj.image\n",
    "        types = EventObj.types\n",
    "        alphas = EventObj.alphas\n",
    "        FLAGS = EventObj.flags\n",
    "        cv2.imshow('Segmented image', output)\n",
    "        cv2.imshow('Input Image', img)\n",
    "        \n",
    "        k = cv2.waitKey(1)\n",
    "\n",
    "        # key bindings\n",
    "        if k == 27:\n",
    "            # esc to exit\n",
    "            break\n",
    "        \n",
    "        elif k == ord('0'): \n",
    "            # Strokes for background\n",
    "            FLAGS['value'] = DRAW_BG\n",
    "        \n",
    "        elif k == ord('1'):\n",
    "            # FG drawing\n",
    "            FLAGS['value'] = DRAW_FG\n",
    "        \n",
    "        elif k == ord('r'):\n",
    "            # reset everything\n",
    "            FLAGS['RECT'] = (0, 0, 1, 1)\n",
    "            FLAGS['DRAW_STROKE'] = False\n",
    "            FLAGS['DRAW_RECT'] = False\n",
    "            FLAGS['rect_or_mask'] = -1\n",
    "            FLAGS['rect_over'] = False\n",
    "            FLAGS['value'] = DRAW_FG\n",
    "            img = img2.copy()\n",
    "            types = np.zeros(img.shape[:2], dtype = np.uint8) \n",
    "            alphas = np.zeros(img.shape[:2], dtype = np.uint8)\n",
    "            EventObj.image = img\n",
    "            EventObj.types = types\n",
    "            EventObj.alphas = alphas\n",
    "            output = np.zeros(img.shape, np.uint8)\n",
    "        \n",
    "        elif k == 13: \n",
    "            # Press carriage return to initiate segmentation\n",
    "            \n",
    "            #-------------------------------------------------#\n",
    "            # Implement GrabCut here.                         #  \n",
    "            # Function should return a mask which can be used #\n",
    "            # to segment the original image as shown on L90   # \n",
    "            #-------------------------------------------------#\n",
    "            alphas = grab_cut(img2, types, alphas, n_components, gamma, lamda, num_iters, tol, connect_diag)\n",
    "            EventObj.alphas = alphas\n",
    "\n",
    "        \n",
    "        EventObj.flags = FLAGS\n",
    "        mask2 = np.where((alphas == 1), 255, 0).astype('uint8')\n",
    "        output = cv2.bitwise_and(img2, img2, mask = mask2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "- We use eight-connectivity by default, calculating $\\beta$ as the inverse of twice the average of $||z_m - z_n||_{2}^{2}$ over all edges in the graph (neighbouring pixels).\n",
    "- The capacity of edges connecting known foreground pixels to the source and known background pixels to the sink is set to $9$ times the value of $\\gamma$, ensuring that it is large enough (for both four- and eight- connectivity) for the edge to never get cut.\n",
    "- We skip the indicator term from the smoothness energy, allowing all neighbouring pixels to be connected, irrespective of whether they are at the object boundary or not. This improves results by allowing pixels in the internal regions of the object to force neighbouring pixels towards itself.\n",
    "- As stated above, we rerun the entire iterative optimisation again after user refinement (which is optional according to the original algorithm), favouring accuracy over runtime.\n",
    "- By default, we use $\\gamma=50$ and $3$ iterations of grabcut, with a tolerance of change for the mincut value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gmms(img, alphas, n_components):\n",
    "    fg = GaussianMixture(n_components=n_components)\n",
    "    fg.fit(img[alphas == con.FG].reshape((-1, img.shape[-1])))\n",
    "\n",
    "    bg = GaussianMixture(n_components=n_components)\n",
    "    bg.fit(img[alphas == con.BG].reshape((-1, img.shape[-1])))\n",
    "\n",
    "    return fg, bg\n",
    "\n",
    "def graph_cut(img, types, alphas, fg_gmm, bg_gmm, beta, gamma, lamda, connect_diag):\n",
    "    logging.info('GRAPH CUT')\n",
    "    \n",
    "    # compute region energy in one go to speed up\n",
    "    fg_D = - fg_gmm.score_samples(img.reshape((-1, img.shape[-1]))).reshape(img.shape[:-1])\n",
    "    bg_D = - bg_gmm.score_samples(img.reshape((-1, img.shape[-1]))).reshape(img.shape[:-1])\n",
    "\n",
    "    # closure function to calculate boundary energy\n",
    "    def compute_V(i, j, oi, oj):\n",
    "        diff = img[i, j] - img[oi, oj]\n",
    "        return gamma * np.exp(- beta * diff.dot(diff))\n",
    "\n",
    "    # fixed capacity for known edges\n",
    "    fix_cap = lamda\n",
    "\n",
    "    # BUILD GRAPH\n",
    "    logging.info('BUILD GRAPH')\n",
    "    num_pix = img.shape[0] * img.shape[1]\n",
    "\n",
    "    def vid(i, j): # vertex ID\n",
    "        return (img.shape[1] * i) + j\n",
    "\n",
    "    def ind(idx): # image index\n",
    "        return ((idx // img.shape[1]), (idx % img.shape[1]))\n",
    "    \n",
    "    graph = ig.Graph(directed=False)\n",
    "    graph.add_vertices(num_pix + 2)\n",
    "    S = num_pix\n",
    "    T = num_pix+1\n",
    "    # the last two vertices are S and T respectively\n",
    "\n",
    "    edges = []\n",
    "    weights = []\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "\n",
    "            # add edges to S and T\n",
    "            if types[i, j] == con.FIX:\n",
    "                if alphas[i, j] == con.FG:\n",
    "                    edges.append((vid(i, j), S))\n",
    "                    weights.append(fix_cap)\n",
    "                else:\n",
    "                    edges.append((vid(i, j), T))\n",
    "                    weights.append(fix_cap)\n",
    "            else:\n",
    "                edges.append((vid(i, j), S))\n",
    "                weights.append(bg_D[i, j])\n",
    "\n",
    "                edges.append((vid(i, j), T))\n",
    "                weights.append(fg_D[i, j])\n",
    "            \n",
    "            # add edges to neighbours\n",
    "            if i > 0:\n",
    "                oi = i-1\n",
    "                oj = j\n",
    "                edges.append((vid(i, j), vid(oi, oj)))\n",
    "                weights.append(compute_V(i, j, oi, oj))\n",
    "            \n",
    "            if j > 0:\n",
    "                oi = i\n",
    "                oj = j-1 \n",
    "                edges.append((vid(i, j), vid(oi, oj)))\n",
    "                weights.append(compute_V(i, j, oi, oj))\n",
    "\n",
    "            if connect_diag:\n",
    "                if i > 0 and j > 0:\n",
    "                    oi = i-1\n",
    "                    oj = j-1 \n",
    "                    edges.append((vid(i, j), vid(oi, oj)))\n",
    "                    weights.append(compute_V(i, j, oi, oj))\n",
    "\n",
    "                if i > 0 and j < img.shape[1] - 1:\n",
    "                    oi = i-1\n",
    "                    oj = j+1 \n",
    "                    edges.append((vid(i, j), vid(oi, oj)))\n",
    "                    weights.append(compute_V(i, j, oi, oj))\n",
    "    \n",
    "    graph.add_edges(edges, attributes={'weight': weights})\n",
    "    logging.info('MINCUT')\n",
    "    cut = graph.st_mincut(S, T, capacity='weight')\n",
    "    bg_vertices = cut.partition[0]\n",
    "    fg_vertices = cut.partition[1]\n",
    "    if S in bg_vertices:\n",
    "        bg_vertices, fg_vertices = fg_vertices, bg_vertices\n",
    "    \n",
    "    new_alphas = np.zeros(img.shape[:2], dtype=np.uint8) # con.BG is filled, zeroes is faster\n",
    "    for v in fg_vertices:\n",
    "        if v not in (S, T):\n",
    "            new_alphas[ind(v)] = 1\n",
    "    return cut.value, new_alphas\n",
    "\n",
    "\n",
    "def grab_cut(img_, types_, alphas_, n_components, gamma, lamda,\n",
    "             num_iters, tol, connect_diag):\n",
    "    \n",
    "    logging.debug('GRAB CUT')\n",
    "    img = img_.copy().astype(np.float32)\n",
    "    types = types_.copy() # types tells whether the pixel is fixed or unknown\n",
    "    alphas = alphas_.copy() # alphas tells whether the pixel is fg or bg according to fixed or current estimate\n",
    "    \n",
    "    # calculate beta\n",
    "    logging.info('CALC BETA')\n",
    "    beta = 0\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            if i > 0:\n",
    "                diff = img[i, j] - img[i-1, j]\n",
    "                beta += diff.dot(diff)\n",
    "            if j > 0:\n",
    "                diff = img[i, j] - img[i, j-1]\n",
    "                beta += diff.dot(diff)\n",
    "            if connect_diag:\n",
    "                if i > 0 and j > 0:\n",
    "                    diff = img[i, j] - img[i-1, j-1]\n",
    "                    beta += diff.dot(diff)\n",
    "                if i > 0 and j < img.shape[1] - 1:\n",
    "                    diff = img[i, j] - img[i-1, j+1]\n",
    "                    beta += diff.dot(diff)\n",
    "    if connect_diag:\n",
    "        beta /= (4 * img.shape[0] * img.shape[1] - 3 * img.shape[0] - 3 * img.shape[1] + 2)\n",
    "    else:\n",
    "        beta /= (2 * img.shape[0] * img.shape[1] - img.shape[0] - img.shape[1])\n",
    "    beta *= 2\n",
    "    beta = 1 / beta\n",
    "    \n",
    "    prev_flow = -1\n",
    "    for _ in tq(range(num_iters)):\n",
    "        fg_gmm, bg_gmm = fit_gmms(img, alphas, n_components)\n",
    "        flow, alphas = graph_cut(img, types, alphas, fg_gmm, bg_gmm, beta, gamma, lamda, connect_diag)\n",
    "    \n",
    "        if prev_flow != -1 and abs(prev_flow - flow) < tol:\n",
    "            break\n",
    "        \n",
    "        prev_flow = flow\n",
    "    \n",
    "    logging.info('DONE')\n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:26<00:00, 28.81s/it]\n",
      "100%|██████████| 3/3 [01:06<00:00, 22.03s/it]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    filename = '../images/banana1.jpg'              # Path to image file\n",
    "    try:\n",
    "        run(filename)\n",
    "    finally:\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Results\n",
    "\n",
    "Here, we vary different parameters to see their effect on the performance of the algorithm qualitatively (due to the absence of ground truth segmentation masks required for any quantitative analysis), which, for this specific problem, easily gives a very good idea of the performance of the algorithm. It is evident from the start that the algorithm can handle some very challenging situations (such as noisy backgrounds and foregrounds) really well, even in just a few iterations and with imperfect user input.\n",
    "\n",
    "**Run-1: ** Default parameters, no user refinement.  \n",
    "<img src=\"../out/results-2.png\" height=300/><img src=\"../out/results-1.png\" height=300/>\n",
    "\n",
    "**Run-2: ** Default parameters, with user refinement.  \n",
    "<img src=\"../out/no_efine_out.png\" width=500/>  \n",
    "<img src=\"../out/results-4.png\" width=500/>  \n",
    "\n",
    "This one is challenging because of the similarity in colours of the foreground and background pixels. The boundary (smoothness) term in the Gibbs Energy plays an important role here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Parameters and Variations\n",
    "\n",
    "### Effect of $\\gamma$\n",
    "$\\gamma$ weights the boundary edges against the region edges. A larger $\\gamma$ makes it harder for boundary edges to be cut, placing more importance on continuity over the individual susceptibility of a pixel to belong to the foreground or background. Here, we run the algorithm with $\\gamma$ set to $50$ on the higher side and $1$ on the lower side on the `llama.jpg` image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:46<00:00, 15.56s/it]\n"
     ]
    }
   ],
   "source": [
    "filename = '../images/llama.jpg'\n",
    "alphas = grab_cut(img2, types, alphas, n_components, gamma, lamda, num_iters, tol, connect_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run 1: ** $\\gamma = 50$  \n",
    "<img src=\"../out/gamma-1.png\" width=500/>\n",
    "\n",
    "**Run 2: ** $\\gamma = 1$  \n",
    "<img src=\"../out/gamma-2.png\" width=500/>\n",
    "\n",
    "Here, we notice that in the second run, a lot of the background which is *similarily coloured as the subject,* is labelled as the foreground. This is because by setting $\\gamma=1$, we lower the weight of the boundary edges, letting them be easily cut. This places more importance on the individual resemblance of a pixel to the mainstream foreground and background colours - allowing parts of the background similarily coloured as the object to be classified as the foreground - even though they are discontinuously placed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of four- and eight- connectivity\n",
    "\n",
    "The graphcut algorithm which is at the heart of grabcut uses a graph model to represent connections between neighbouring pixels. The neighbourhood of a pixel can be defined as the four or pixels surrounding it, among other ways. Which method is used affects the strength of boundary relations - which play an important role in the tendency of the algorithm to categorise continuous regions similarly. The two-fold change in number of edges in the graph affects the time taken by the mincut algorithm to run as well. Here we run the algorithm with four- and eight- connectivity on `llama.jpg` with a reduced value of $\\gamma$ to highten the visibility of the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:47<00:00, 15.99s/it]\n",
      "100%|██████████| 3/3 [00:45<00:00, 15.11s/it]\n"
     ]
    }
   ],
   "source": [
    "filename = '../images/llama.jpg'\n",
    "try:\n",
    "    run(filename, gamma=10, connect_diag=True)\n",
    "    run(filename, gamma=10, connect_diag=False)\n",
    "finally:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run 1: Eight-Connectivity**  \n",
    "<img src=\"../out/connect-1.png\" width=500/>\n",
    "\n",
    "**Run 2: Four-Connectivity**  \n",
    "<img src=\"../out/connect-2.png\" width=500/>\n",
    "\n",
    "We again notice a tendency of regions of the background similar in colour to the object being categorised as the foreground. This is because a decrease in number of edges and connectivity of the graph places more weight on the region terms of the energy function, giving more importance to the colour (and hence likeness to the background and foreground colour models) of an individual pixel rather than its relation to the surrounding region. We also notice noisy and rough edges because of the same effect - each pixel's individual colour becomes more important. This is further visible in the segmentation of the image `elefant.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:57<00:00, 19.01s/it]\n",
      "100%|██████████| 3/3 [00:39<00:00, 13.26s/it]\n"
     ]
    }
   ],
   "source": [
    "filename = '../images/elefant.jpg'\n",
    "try:\n",
    "    run(filename, gamma=10, connect_diag=True)\n",
    "    run(filename, gamma=10, connect_diag=False)\n",
    "finally:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run 1: Eight-Connectivity**  \n",
    "<img src=\"../out/connect-3.png\" width=500/>\n",
    "\n",
    "**Run 2: Four-Connectivity**  \n",
    "<img src=\"../out/connect-4.png\" width=500/>\n",
    "\n",
    "Notice the rough edges in the second case, with four-connectivity. This artifact arises because whether or not two diagonally adjacent pixels are categorised similarly has no affect on the energy and hence the segmentation - giving the edges a staircase-like look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Number of Components in the Gaussian Mixture Foreground and Background Models\n",
    "\n",
    "We use two gaussian mixture models, one for the background and the other for the foreground. Each component in the model allows it to represent a different colour shade which could appear in the background or the foreground. The intent here is to have enough components to represent colours which form major sections of the background/foreground, such as the sky, grass and street in the background of a road scene, and the red body, blue windows and black tires of a car in the foreground. Small regions of a different colour need not be explicitly modelled by a gaussian mixture component, as they can be handled by the boundary terms alone. While increasing the number of components allows for the mixtures to model more diverse foregrounds and backgrounds, it could also lead to the two mixtures modelling components of the other because of imperfect $\\alpha$s while initialising the models - leading to the foreground including parts of the background and vice versa. The time required to fit the mixture models also increases with an increase in number of components. Here we try gaussian mixtures with $5$ and $10$ components on an image with a diverse background, `person6.jpg`. The size of the bounding box is kept large to highten the difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:50<00:00, 16.80s/it]\n",
      "100%|██████████| 3/3 [01:27<00:00, 29.16s/it]\n"
     ]
    }
   ],
   "source": [
    "filename = '../images/person6.jpg'\n",
    "try:\n",
    "    run(filename, n_components=5)\n",
    "    run(filename, n_components=10)\n",
    "finally:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run 1: Number of Components**  \n",
    "<img src=\"../out/components-1.png\" width=500/>\n",
    "\n",
    "**Run 2: Number of Components**  \n",
    "<img src=\"../out/components-2.png\" width=500/>\n",
    "\n",
    "While none of the segmentations is perfect, the second one has more of the background being classified as foreground - because of the foreground gaussian mixture modelling colours that are part of the actual background. The runtime difference is also noticeable, with each iteration taking a little less than twice the time on doubling the number of mixture components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
